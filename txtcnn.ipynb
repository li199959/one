{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCDTW9o7PrNSXbxRzjhae5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li199959/one/blob/main/txtcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWlp8lB4q-yS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "dtype = torch.FloatTensor\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
        "labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
        "# TextCNN Parameter\n",
        "embedding_size = 2 # wordemb dim\n",
        "sequence_length = len(sentences[0]) # every sentences contains sequence_length(=3) words\n",
        "num_classes = len(set(labels))  # 0 or 1\n",
        "batch_size = 3\n",
        "word_list = \" \".join(sentences).split()\n",
        "vocab = list(set(word_list))\n",
        "vocab"
      ],
      "metadata": {
        "id": "KvQsRiyUrEmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "word2idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY3k9ElKrNX3",
        "outputId": "8c68da7d-0e87-490f-ca8c-694662490ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'baseball': 0,\n",
              " 'love': 1,\n",
              " 'she': 2,\n",
              " 'i': 3,\n",
              " 'you': 4,\n",
              " 'this': 5,\n",
              " 'for': 6,\n",
              " 'me': 7,\n",
              " 'hate': 8,\n",
              " 'is': 9,\n",
              " 'that': 10,\n",
              " 'sorry': 11,\n",
              " 'loves': 12,\n",
              " 'awful': 13,\n",
              " 'likes': 14,\n",
              " 'he': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DYUlVDcrVw0",
        "outputId": "f143c285-2017-4b16-e0e4-5e6820471a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data(sentences, labels):\n",
        "  inputs = []\n",
        "  for sen in sentences:\n",
        "      inputs.append([word2idx[n] for n in sen.split()])\n",
        "\n",
        "  targets = []\n",
        "  for out in labels:\n",
        "      targets.append(out) # To using Torch Softmax Loss function\n",
        "  return inputs, targets\n",
        "\n",
        "input_batch, target_batch = make_data(sentences, labels)\n",
        "input_batch, target_batch = torch.LongTensor(input_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "dataset = Data.TensorDataset(input_batch, target_batch)\n",
        "loader = Data.DataLoader(dataset, batch_size, True)"
      ],
      "metadata": {
        "id": "-Aszd3XCrjWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6KTYknMr8e0",
        "outputId": "f555093a-fde3-4806-dc32-21773d449078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.TensorDataset at 0x7f70969eae90>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJnFxQxksAYf",
        "outputId": "18f03ebb-be3f-4b3c-e701-7d8c720c83a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3,  1,  4],\n",
              "        [15, 12,  7],\n",
              "        [ 2, 14,  0],\n",
              "        [ 3,  8,  4],\n",
              "        [11,  6, 10],\n",
              "        [ 5,  9, 13]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.W = nn.Embedding(vocab_size, embedding_size)\n",
        "        output_channel = 3\n",
        "        self.conv = nn.Sequential(\n",
        "            # conv : [input_channel(=1), output_channel, (filter_height, filter_width), stride=1]\n",
        "            nn.Conv2d(1, output_channel, (2, embedding_size)), # => [batch_size, output_channel, 2, 1]\n",
        "            nn.ReLU(),\n",
        "            # pool : ((filter_height, filter_width))\n",
        "            nn.MaxPool2d((2, 1)),\n",
        "        )\n",
        "        # fc\n",
        "        self.fc = nn.Linear(output_channel, num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "      '''\n",
        "      X: [batch_size, sequence_length]\n",
        "      '''\n",
        "      batch_size = X.shape[0]\n",
        "      embedding_X = self.W(X) # [batch_size, sequence_length, embedding_size]\n",
        "      embedding_X = embedding_X.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n",
        "      conved = self.conv(embedding_X) # [batch_size, output_channel, 1, 1]\n",
        "      flatten = conved.view(batch_size, -1) # [batch_size, output_channel*1*1]\n",
        "      output = self.fc(flatten)\n",
        "      return output"
      ],
      "metadata": {
        "id": "KWWQGZr9sF1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.W = nn.Embedding(vocab_size, embedding_size)\n",
        "        output_channel = 3\n",
        "        self.conv = nn.Sequential(\n",
        "            # conv : [input_channel(=1), output_channel, (filter_height, filter_width), stride=1]\n",
        "            nn.Conv2d(1, output_channel, (2, embedding_size)), # => [batch_size, output_channel, 2, 1]\n",
        "            nn.ReLU(),\n",
        "            # pool : ((filter_height, filter_width))\n",
        "            nn.MaxPool2d((2, 1)),\n",
        "        )\n",
        "        # fc\n",
        "        self.fc = nn.Linear(output_channel, num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "      '''\n",
        "      X: [batch_size, sequence_length]\n",
        "      '''\n",
        "      batch_size = X.shape[0]\n",
        "      embedding_X = self.W(X) # [batch_size, sequence_length, embedding_size]\n",
        "      embedding_X = embedding_X.unsqueeze(1) # add channel(=1) [batch, channel(=1), sequence_length, embedding_size]\n",
        "      conved = self.conv(embedding_X) # [batch_size, output_channel, 1, 1]\n",
        "      flatten = conved.view(batch_size, -1) # [batch_size, output_channel*1*1]\n",
        "      output = self.fc(flatten)\n",
        "      return output"
      ],
      "metadata": {
        "id": "YHu56Fr6uQY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training\n",
        "for epoch in range(5000):\n",
        "  for batch_x, batch_y in loader:\n",
        "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "    pred = model(batch_x)\n",
        "    loss = criterion(pred, batch_y)\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "awWAc0xCxcWD",
        "outputId": "49555c26-ac83-4e8b-acae-f77d9d2026eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1000 loss = 0.102257\n",
            "Epoch: 1000 loss = 0.007969\n",
            "Epoch: 2000 loss = 0.000502\n",
            "Epoch: 2000 loss = 0.030564\n",
            "Epoch: 3000 loss = 0.010610\n",
            "Epoch: 3000 loss = 0.000199\n",
            "Epoch: 4000 loss = 0.004042\n",
            "Epoch: 4000 loss = 0.000064\n",
            "Epoch: 5000 loss = 0.001605\n",
            "Epoch: 5000 loss = 0.000015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "test_text = 'i hate me'\n",
        "tests = [[word2idx[n] for n in test_text.split()]]\n",
        "test_batch = torch.LongTensor(tests).to(device)\n",
        "\n",
        "# Predict\n",
        "model = model.eval()\n",
        "predict = model(test_batch).data.max(1, keepdim=True)[1]\n",
        "if predict[0][0] == 0:\n",
        "    print(test_text,\"is Bad Mean...\")\n",
        "else:\n",
        "    print(test_text,\"is Good Mean!!\")"
      ],
      "metadata": {
        "id": "8RYH3JTuxrFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61136ca6-8ea2-4937-997c-5964a3f9f17e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i hate me is Bad Mean...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rxdMWYOvzja-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}