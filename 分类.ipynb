{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrUwp5PDgtCRMmLfUZsNPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li199959/one/blob/main/%E5%88%86%E7%B1%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qMZ0Ebv5AZzX"
      },
      "outputs": [],
      "source": [
        "from pickletools import optimize\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "SaWS5wFFAjg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'toutiao_cat_data.txt'"
      ],
      "metadata": {
        "id": "1cvwBdHJAcfH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToutiaoDataset(data.Dataset):\n",
        "    def __init__(self, data_path) -> None:\n",
        "        super(ToutiaoDataset, self).__init__()\n",
        "        self.build(data_path)\n",
        "\n",
        "    def build(self, data_path):\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            texts = []\n",
        "            labels = []\n",
        "            for line in f:\n",
        "                    _, category, _, text, key_word = line.strip().split('_!_')\n",
        "\n",
        "                    labels.append(int(category)-100)\n",
        "                    texts.append(text)\n",
        "                    # print(texts[-1], labels[-1]);input()\n",
        "            self.texts = texts\n",
        "            self.labels = labels\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        text_i = self.texts[i]\n",
        "        label_i = self.labels[i]\n",
        "        return text_i, label_i\n",
        "\n",
        "toutiao_dataset = ToutiaoDataset(data_path)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "def collate_fn(text_label):\n",
        "    #text_label =[\n",
        "    # ('我是大肥猪', 0), \n",
        "    # ('我是小肥猪', 1),\n",
        "    # ('兔子好可爱', 2)\n",
        "    # ]\n",
        "    texts = [text for text, _ in text_label]\n",
        "    labels = [label for _, label in text_label]\n",
        "    max_len = min(max(len(t) for t in texts) + 2, 512)\n",
        "\n",
        "    # texts = [\n",
        "    #     '我是大肥猪',\n",
        "    #     '我是小肥猪',\n",
        "    #     '兔子好可爱'\n",
        "    # ]\n",
        "    data = bert_tokenizer.batch_encode_plus(\n",
        "        batch_text_or_text_pairs=texts,\n",
        "        add_special_tokens=True,\n",
        "\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_len,\n",
        "        return_tensors='pt',#tf,pt,np\n",
        "\n",
        "        return_token_type_ids=True,\n",
        "        return_attention_mask=True,\n",
        "        return_special_tokens_mask=True,\n",
        "    )\n",
        "    # for key, value in data.items():\n",
        "    #     print(key, ':', value);input()\n",
        "\n",
        "    input_ids = data['input_ids']\n",
        "    attention_mask = data['attention_mask']\n",
        "    token_type_ids = data['token_type_ids']\n",
        "    labels = torch.LongTensor(labels)\n",
        "    return input_ids, attention_mask, token_type_ids, labels\n",
        "\n",
        "\n",
        "#(batch_size, seq_len, hidden_size)\n",
        "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
        "# pretrained = BertModel()\n",
        "#pretrained.load_state_dict(xx)\n",
        "class ToutiaoClassification(nn.Module):\n",
        "    def __init__(self, hidden_size, num_classes) -> None:\n",
        "        super(ToutiaoClassification, self).__init__()\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        with torch.no_grad():\n",
        "            out = pretrained(input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids)\n",
        "        #fc(.shape = (batch_size, hidden_size))\n",
        "        prediction = self.fc(out.last_hidden_state[:, 0, :])\n",
        "        return prediction\n",
        "\n",
        "lr = 5e-4\n",
        "batch_size = 32\n",
        "shuffle = True\n",
        "epochs = 4\n",
        "\n",
        "dataloader = data.DataLoader(toutiao_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "model = ToutiaoClassification(hidden_size=768, num_classes=17)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    counter = 0\n",
        "    for input_ids, attention_mask, token_type_ids, label in dataloader:\n",
        "        prediciton = model(input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids)\n",
        "        \n",
        "        loss = criterion(prediciton, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        prediciton = prediciton.argmax(dim=1)\n",
        "        accuracy = (prediciton == label).sum().item() / len(label)\n",
        "        counter += 1\n",
        "\n",
        "        print(counter, loss.item(), accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y8a0cvhdBYYf",
        "outputId": "e698e778-9703-4012-9e3d-4200e4e0c02b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 3.0365872383117676 0.0\n",
            "2 2.798135280609131 0.03125\n",
            "3 2.816507577896118 0.0625\n",
            "4 2.7371623516082764 0.15625\n",
            "5 2.7119100093841553 0.125\n",
            "6 2.532694101333618 0.21875\n",
            "7 2.6986520290374756 0.09375\n",
            "8 2.648597240447998 0.125\n",
            "9 2.7281436920166016 0.125\n",
            "10 2.474766731262207 0.25\n",
            "11 2.5555312633514404 0.1875\n",
            "12 2.406691789627075 0.375\n",
            "13 2.500128746032715 0.25\n",
            "14 2.592433452606201 0.1875\n",
            "15 2.3990066051483154 0.21875\n",
            "16 2.40392804145813 0.3125\n",
            "17 2.2900640964508057 0.3125\n",
            "18 2.4136197566986084 0.21875\n",
            "19 2.309659957885742 0.3125\n",
            "20 2.2608439922332764 0.3125\n",
            "21 2.231853723526001 0.28125\n",
            "22 2.2815401554107666 0.28125\n",
            "23 2.226658821105957 0.34375\n",
            "24 2.1324214935302734 0.3125\n",
            "25 2.2456893920898438 0.3125\n",
            "26 2.373019218444824 0.3125\n",
            "27 2.044534683227539 0.53125\n",
            "28 2.086271286010742 0.3125\n",
            "29 2.1630616188049316 0.375\n",
            "30 2.0102007389068604 0.53125\n",
            "31 2.0341320037841797 0.5625\n",
            "32 2.194951057434082 0.5\n",
            "33 2.0136704444885254 0.5\n",
            "34 2.1275453567504883 0.4375\n",
            "35 2.1159725189208984 0.5\n",
            "36 2.086566925048828 0.40625\n",
            "37 2.019089698791504 0.59375\n",
            "38 1.8243536949157715 0.65625\n",
            "39 1.8997236490249634 0.65625\n",
            "40 2.0852389335632324 0.53125\n",
            "41 2.0272483825683594 0.5625\n",
            "42 1.8368231058120728 0.6875\n",
            "43 1.939164400100708 0.625\n",
            "44 1.9666738510131836 0.5\n",
            "45 1.724030613899231 0.75\n",
            "46 1.858843207359314 0.65625\n",
            "47 1.7029526233673096 0.625\n",
            "48 1.7331746816635132 0.5625\n",
            "49 1.7741118669509888 0.625\n",
            "50 1.906524896621704 0.46875\n",
            "51 1.6082258224487305 0.75\n",
            "52 1.863815426826477 0.53125\n",
            "53 2.073618173599243 0.53125\n",
            "54 1.4998222589492798 0.75\n",
            "55 1.8845837116241455 0.53125\n",
            "56 1.626183032989502 0.6875\n",
            "57 1.938913345336914 0.5625\n",
            "58 1.6421231031417847 0.8125\n",
            "59 1.8307626247406006 0.53125\n",
            "60 1.6705801486968994 0.6875\n",
            "61 1.4930528402328491 0.75\n",
            "62 1.492108702659607 0.65625\n",
            "63 1.8207279443740845 0.53125\n",
            "64 1.4620826244354248 0.75\n",
            "65 1.5203287601470947 0.65625\n",
            "66 1.7447192668914795 0.53125\n",
            "67 1.5334166288375854 0.75\n",
            "68 1.5972415208816528 0.65625\n",
            "69 1.5277562141418457 0.71875\n",
            "70 1.5484548807144165 0.65625\n",
            "71 1.5416733026504517 0.6875\n",
            "72 1.5442100763320923 0.65625\n",
            "73 1.534827470779419 0.65625\n",
            "74 1.373037576675415 0.71875\n",
            "75 1.9412795305252075 0.625\n",
            "76 1.7077473402023315 0.53125\n",
            "77 1.5024508237838745 0.59375\n",
            "78 1.5636887550354004 0.65625\n",
            "79 1.3799055814743042 0.84375\n",
            "80 1.4472198486328125 0.71875\n",
            "81 1.4036351442337036 0.75\n",
            "82 1.4495805501937866 0.71875\n",
            "83 1.5593186616897583 0.6875\n",
            "84 1.6414666175842285 0.6875\n",
            "85 1.5538090467453003 0.75\n",
            "86 1.466020107269287 0.8125\n",
            "87 1.3593884706497192 0.78125\n",
            "88 1.5625457763671875 0.65625\n",
            "89 1.6328792572021484 0.625\n",
            "90 1.5447684526443481 0.6875\n",
            "91 1.2778830528259277 0.78125\n",
            "92 1.4526399374008179 0.65625\n",
            "93 1.4846962690353394 0.625\n",
            "94 1.1694895029067993 0.78125\n",
            "95 1.53731369972229 0.59375\n",
            "96 1.4444847106933594 0.65625\n",
            "97 1.2552608251571655 0.6875\n",
            "98 1.3360432386398315 0.8125\n",
            "99 1.6566145420074463 0.5\n",
            "100 1.236276388168335 0.75\n",
            "101 1.195970058441162 0.65625\n",
            "102 1.2030707597732544 0.71875\n",
            "103 1.3786860704421997 0.71875\n",
            "104 1.1828618049621582 0.75\n",
            "105 1.0864908695220947 0.75\n",
            "106 1.1694031953811646 0.84375\n",
            "107 1.1215341091156006 0.71875\n",
            "108 1.2302993535995483 0.75\n",
            "109 1.5088478326797485 0.53125\n",
            "110 1.213591456413269 0.71875\n",
            "111 1.2062759399414062 0.78125\n",
            "112 1.3138877153396606 0.65625\n",
            "113 1.4986543655395508 0.5\n",
            "114 1.4522335529327393 0.59375\n",
            "115 1.3392324447631836 0.625\n",
            "116 1.3677841424942017 0.625\n",
            "117 1.2753831148147583 0.71875\n",
            "118 1.2493774890899658 0.71875\n",
            "119 1.2279002666473389 0.78125\n",
            "120 1.2417254447937012 0.71875\n",
            "121 1.4249238967895508 0.65625\n",
            "122 1.0798263549804688 0.71875\n",
            "123 1.186061978340149 0.8125\n",
            "124 1.0942811965942383 0.78125\n",
            "125 1.2948476076126099 0.84375\n",
            "126 1.0725324153900146 0.8125\n",
            "127 1.4326268434524536 0.59375\n",
            "128 1.0873849391937256 0.8125\n",
            "129 0.9907886981964111 0.78125\n",
            "130 1.1437687873840332 0.78125\n",
            "131 1.1758520603179932 0.71875\n",
            "132 1.1313982009887695 0.75\n",
            "133 1.209848165512085 0.78125\n",
            "134 1.509840726852417 0.65625\n",
            "135 1.0843465328216553 0.8125\n",
            "136 1.2032825946807861 0.65625\n",
            "137 1.2218959331512451 0.71875\n",
            "138 1.0411951541900635 0.8125\n",
            "139 1.3160501718521118 0.65625\n",
            "140 0.9113312363624573 0.90625\n",
            "141 1.2587172985076904 0.65625\n",
            "142 0.9541985988616943 0.71875\n",
            "143 1.4318568706512451 0.5625\n",
            "144 1.0471713542938232 0.75\n",
            "145 1.1656111478805542 0.59375\n",
            "146 1.3146435022354126 0.625\n",
            "147 0.8547406196594238 0.8125\n",
            "148 1.1251167058944702 0.71875\n",
            "149 1.3377562761306763 0.6875\n",
            "150 1.1034514904022217 0.75\n",
            "151 1.0019664764404297 0.90625\n",
            "152 1.0939264297485352 0.8125\n",
            "153 1.2202733755111694 0.65625\n",
            "154 1.1949372291564941 0.75\n",
            "155 0.8292937278747559 0.90625\n",
            "156 1.2285144329071045 0.6875\n",
            "157 1.026702642440796 0.8125\n",
            "158 0.9932448863983154 0.8125\n",
            "159 0.9264832139015198 0.8125\n",
            "160 1.1095349788665771 0.75\n",
            "161 1.4435834884643555 0.71875\n",
            "162 1.0726348161697388 0.6875\n",
            "163 0.9126964211463928 0.8125\n",
            "164 1.0318986177444458 0.8125\n",
            "165 1.3921313285827637 0.75\n",
            "166 1.1908619403839111 0.6875\n",
            "167 1.091930866241455 0.6875\n",
            "168 1.0452500581741333 0.84375\n",
            "169 1.0225512981414795 0.71875\n",
            "170 0.7686337828636169 0.9375\n",
            "171 1.2234344482421875 0.65625\n",
            "172 0.996019184589386 0.6875\n",
            "173 1.0427508354187012 0.75\n",
            "174 1.0148200988769531 0.75\n",
            "175 1.3243675231933594 0.71875\n",
            "176 0.9184561371803284 0.8125\n",
            "177 1.0272620916366577 0.71875\n",
            "178 0.9337922930717468 0.84375\n",
            "179 1.2039906978607178 0.78125\n",
            "180 0.8159883618354797 0.84375\n",
            "181 0.5682281851768494 0.9375\n",
            "182 0.7811757326126099 0.875\n",
            "183 0.9650505781173706 0.8125\n",
            "184 0.7838698029518127 0.84375\n",
            "185 0.7295295596122742 0.875\n",
            "186 1.0462814569473267 0.71875\n",
            "187 0.9848257303237915 0.78125\n",
            "188 1.006900429725647 0.75\n",
            "189 1.3017326593399048 0.5625\n",
            "190 1.0604727268218994 0.78125\n",
            "191 1.0656287670135498 0.6875\n",
            "192 1.0463743209838867 0.6875\n",
            "193 0.9992398619651794 0.75\n",
            "194 1.1083106994628906 0.71875\n",
            "195 0.8229018449783325 0.78125\n",
            "196 1.1474136114120483 0.78125\n",
            "197 0.8830527067184448 0.75\n",
            "198 0.9505846500396729 0.8125\n",
            "199 0.9414176344871521 0.84375\n",
            "200 1.0765026807785034 0.78125\n",
            "201 1.1437288522720337 0.71875\n",
            "202 0.9901148080825806 0.8125\n",
            "203 0.8398500680923462 0.8125\n",
            "204 0.7792955636978149 0.78125\n",
            "205 1.087076187133789 0.8125\n",
            "206 0.9313887357711792 0.875\n",
            "207 1.1906336545944214 0.71875\n",
            "208 0.815094530582428 0.875\n",
            "209 0.9555279612541199 0.8125\n",
            "210 0.9671348333358765 0.78125\n",
            "211 1.0105113983154297 0.8125\n",
            "212 0.9224096536636353 0.6875\n",
            "213 0.8631532788276672 0.75\n",
            "214 0.8197596669197083 0.8125\n",
            "215 0.8559705018997192 0.8125\n",
            "216 0.9021081328392029 0.71875\n",
            "217 1.091705083847046 0.78125\n",
            "218 1.0280262231826782 0.78125\n",
            "219 0.8267274498939514 0.78125\n",
            "220 0.8262753486633301 0.8125\n",
            "221 0.8346427083015442 0.84375\n",
            "222 1.1470632553100586 0.78125\n",
            "223 1.0941808223724365 0.71875\n",
            "224 1.0135856866836548 0.71875\n",
            "225 0.8234904408454895 0.8125\n",
            "226 0.8446524739265442 0.84375\n",
            "227 0.8606550097465515 0.8125\n",
            "228 1.0148942470550537 0.75\n",
            "229 1.0830106735229492 0.6875\n",
            "230 0.8212890625 0.8125\n",
            "231 0.7966649532318115 0.78125\n",
            "232 0.8559024930000305 0.84375\n",
            "233 1.166134238243103 0.5625\n",
            "234 1.0250003337860107 0.65625\n",
            "235 0.8324530124664307 0.78125\n",
            "236 0.9538316130638123 0.78125\n",
            "237 0.7737383842468262 0.84375\n",
            "238 0.9114592671394348 0.75\n",
            "239 1.0072561502456665 0.75\n",
            "240 0.8124868869781494 0.8125\n",
            "241 1.4792208671569824 0.46875\n",
            "242 0.9563004374504089 0.65625\n",
            "243 0.7791934013366699 0.875\n",
            "244 0.7096679210662842 0.875\n",
            "245 0.7918987274169922 0.78125\n",
            "246 0.8236901164054871 0.75\n",
            "247 0.918983519077301 0.75\n",
            "248 0.8710281252861023 0.8125\n",
            "249 0.7085933089256287 0.96875\n",
            "250 0.8082088232040405 0.75\n",
            "251 0.790927529335022 0.84375\n",
            "252 0.8368914723396301 0.71875\n",
            "253 0.996734082698822 0.71875\n",
            "254 0.7764822840690613 0.8125\n",
            "255 0.9115365743637085 0.75\n",
            "256 0.6205845475196838 0.90625\n",
            "257 0.9212144017219543 0.8125\n",
            "258 0.9937065839767456 0.78125\n",
            "259 0.9635480642318726 0.6875\n",
            "260 1.089463472366333 0.625\n",
            "261 0.7933587431907654 0.8125\n",
            "262 0.9080430865287781 0.71875\n",
            "263 1.0064595937728882 0.875\n",
            "264 0.9542315006256104 0.78125\n",
            "265 1.068402647972107 0.71875\n",
            "266 1.1253896951675415 0.75\n",
            "267 1.015765905380249 0.84375\n",
            "268 0.7414224743843079 0.875\n",
            "269 0.9309062957763672 0.875\n",
            "270 0.8836348652839661 0.75\n",
            "271 0.6571687459945679 0.84375\n",
            "272 0.968012809753418 0.71875\n",
            "273 1.023801565170288 0.75\n",
            "274 0.9667406678199768 0.71875\n",
            "275 0.7656113505363464 0.875\n",
            "276 1.1026110649108887 0.65625\n",
            "277 1.0829535722732544 0.6875\n",
            "278 0.9630597233772278 0.71875\n",
            "279 0.6581853628158569 0.875\n",
            "280 0.6138805150985718 0.84375\n",
            "281 1.1402924060821533 0.6875\n",
            "282 1.030502200126648 0.75\n",
            "283 0.5664857625961304 0.875\n",
            "284 0.9262016415596008 0.78125\n",
            "285 0.8882597088813782 0.65625\n",
            "286 0.8703118562698364 0.8125\n",
            "287 0.9754207730293274 0.71875\n",
            "288 0.8882104158401489 0.78125\n",
            "289 1.0032203197479248 0.75\n",
            "290 0.9721021056175232 0.75\n",
            "291 0.818507730960846 0.84375\n",
            "292 1.0660377740859985 0.625\n",
            "293 1.0545891523361206 0.8125\n",
            "294 0.9672574996948242 0.6875\n",
            "295 0.9478836059570312 0.78125\n",
            "296 0.9182195067405701 0.71875\n",
            "297 0.6278715133666992 0.90625\n",
            "298 0.6497611403465271 0.875\n",
            "299 0.7818038463592529 0.78125\n",
            "300 0.630759060382843 0.84375\n",
            "301 0.48756715655326843 0.875\n",
            "302 0.5707117319107056 0.9375\n",
            "303 0.9293141961097717 0.6875\n",
            "304 0.7828322649002075 0.84375\n",
            "305 0.666323721408844 0.90625\n",
            "306 0.8990839719772339 0.8125\n",
            "307 0.8832147121429443 0.84375\n",
            "308 0.8685857653617859 0.8125\n",
            "309 0.7456672787666321 0.875\n",
            "310 0.9154559373855591 0.84375\n",
            "311 0.8884222507476807 0.71875\n",
            "312 0.49433964490890503 0.875\n",
            "313 0.8040012717247009 0.78125\n",
            "314 0.583835780620575 0.875\n",
            "315 0.6579077839851379 0.84375\n",
            "316 0.47220486402511597 0.9375\n",
            "317 0.7341137528419495 0.84375\n",
            "318 0.7137078046798706 0.875\n",
            "319 0.7440635561943054 0.8125\n",
            "320 0.7134985327720642 0.84375\n",
            "321 0.6085746884346008 0.875\n",
            "322 0.7663038372993469 0.78125\n",
            "323 0.7294096350669861 0.90625\n",
            "324 0.9979087114334106 0.65625\n",
            "325 0.8856469392776489 0.75\n",
            "326 0.5831230878829956 0.875\n",
            "327 0.6842458844184875 0.90625\n",
            "328 0.7911962866783142 0.8125\n",
            "329 1.0126351118087769 0.6875\n",
            "330 0.44314420223236084 0.9375\n",
            "331 1.0714480876922607 0.6875\n",
            "332 0.7409466505050659 0.78125\n",
            "333 0.5678575038909912 0.875\n",
            "334 0.8137946724891663 0.78125\n",
            "335 0.7685566544532776 0.84375\n",
            "336 0.8846558928489685 0.75\n",
            "337 0.7235722541809082 0.84375\n",
            "338 1.0125669240951538 0.71875\n",
            "339 0.5464600920677185 0.875\n",
            "340 0.7946666479110718 0.75\n",
            "341 0.5676923990249634 0.875\n",
            "342 0.6760891079902649 0.8125\n",
            "343 0.6561527252197266 0.8125\n",
            "344 0.7730039954185486 0.84375\n",
            "345 0.876182496547699 0.75\n",
            "346 0.6374727487564087 0.875\n",
            "347 0.6799920201301575 0.8125\n",
            "348 0.8623021245002747 0.8125\n",
            "349 0.6194890141487122 0.84375\n",
            "350 0.7961952686309814 0.84375\n",
            "351 0.6805428266525269 0.78125\n",
            "352 0.46111276745796204 0.90625\n",
            "353 0.7429258227348328 0.75\n",
            "354 0.7656848430633545 0.8125\n",
            "355 0.7146670818328857 0.71875\n",
            "356 0.8029792904853821 0.84375\n",
            "357 0.7675447463989258 0.84375\n",
            "358 0.5466744303703308 0.875\n",
            "359 0.4594446122646332 0.90625\n",
            "360 0.4798995852470398 0.875\n",
            "361 0.7229607701301575 0.75\n",
            "362 0.5702818632125854 0.84375\n",
            "363 0.9205448627471924 0.78125\n",
            "364 1.4147926568984985 0.65625\n",
            "365 0.5716981887817383 0.875\n",
            "366 0.5607271194458008 0.84375\n",
            "367 0.7812641859054565 0.78125\n",
            "368 0.5554721355438232 0.875\n",
            "369 0.6422692537307739 0.8125\n",
            "370 0.7651894092559814 0.84375\n",
            "371 0.8113749027252197 0.78125\n",
            "372 0.5674721598625183 0.875\n",
            "373 0.8979885578155518 0.8125\n",
            "374 0.7830835580825806 0.78125\n",
            "375 0.9549015164375305 0.78125\n",
            "376 0.5941643714904785 0.8125\n",
            "377 0.8923379182815552 0.78125\n",
            "378 0.676632285118103 0.875\n",
            "379 0.7484437823295593 0.78125\n",
            "380 0.7147298455238342 0.78125\n",
            "381 0.40939947962760925 0.90625\n",
            "382 0.7978454232215881 0.8125\n",
            "383 0.7686496376991272 0.8125\n",
            "384 1.0650634765625 0.78125\n",
            "385 0.5911772847175598 0.875\n",
            "386 0.7210564017295837 0.84375\n",
            "387 0.821968138217926 0.8125\n",
            "388 0.6569159030914307 0.84375\n",
            "389 1.0625381469726562 0.6875\n",
            "390 0.6808925867080688 0.84375\n",
            "391 0.7761635780334473 0.78125\n",
            "392 0.5709423422813416 0.90625\n",
            "393 0.6431339979171753 0.84375\n",
            "394 0.8497823476791382 0.84375\n",
            "395 0.6359277367591858 0.78125\n",
            "396 0.9203148484230042 0.71875\n",
            "397 0.563345193862915 0.84375\n",
            "398 0.821079432964325 0.71875\n",
            "399 0.9760960340499878 0.6875\n",
            "400 0.8520069718360901 0.8125\n",
            "401 0.6392458081245422 0.8125\n",
            "402 0.6885435581207275 0.875\n",
            "403 0.6273344159126282 0.8125\n",
            "404 0.8325267434120178 0.78125\n",
            "405 0.7741332650184631 0.8125\n",
            "406 0.9530461430549622 0.75\n",
            "407 0.8335283398628235 0.71875\n",
            "408 0.7330009341239929 0.78125\n",
            "409 0.9180378317832947 0.75\n",
            "410 0.9919681549072266 0.8125\n",
            "411 0.8479220867156982 0.78125\n",
            "412 0.4681221544742584 0.875\n",
            "413 0.9164416193962097 0.75\n",
            "414 0.3479249179363251 0.96875\n",
            "415 0.8726110458374023 0.78125\n",
            "416 1.1311901807785034 0.65625\n",
            "417 0.7488014698028564 0.8125\n",
            "418 0.6439274549484253 0.84375\n",
            "419 0.8173278570175171 0.71875\n",
            "420 1.1562453508377075 0.65625\n",
            "421 0.5321253538131714 0.875\n",
            "422 0.9437032341957092 0.71875\n",
            "423 0.5761379599571228 0.875\n",
            "424 0.5186808705329895 0.9375\n",
            "425 0.9425017833709717 0.6875\n",
            "426 0.9642801880836487 0.65625\n",
            "427 0.8030369281768799 0.84375\n",
            "428 0.5970844626426697 0.84375\n",
            "429 0.5029895305633545 0.8125\n",
            "430 0.6559538841247559 0.875\n",
            "431 0.9075201749801636 0.84375\n",
            "432 0.8854446411132812 0.75\n",
            "433 0.6217782497406006 0.84375\n",
            "434 0.9333733320236206 0.6875\n",
            "435 0.6507896184921265 0.75\n",
            "436 0.7644490003585815 0.8125\n",
            "437 0.9651076197624207 0.78125\n",
            "438 0.8037487864494324 0.71875\n",
            "439 0.5803717970848083 0.875\n",
            "440 0.6341609358787537 0.84375\n",
            "441 0.7283810973167419 0.75\n",
            "442 0.6173865795135498 0.84375\n",
            "443 0.8046199679374695 0.78125\n",
            "444 1.1404985189437866 0.71875\n",
            "445 0.8176810145378113 0.78125\n",
            "446 0.8029233813285828 0.75\n",
            "447 0.6150732040405273 0.8125\n",
            "448 0.4384280741214752 0.90625\n",
            "449 0.5541450381278992 0.875\n",
            "450 1.1658958196640015 0.65625\n",
            "451 0.8277559280395508 0.78125\n",
            "452 0.42854025959968567 0.90625\n",
            "453 0.8356201648712158 0.78125\n",
            "454 0.4517025947570801 0.875\n",
            "455 1.1159995794296265 0.65625\n",
            "456 0.9273080229759216 0.71875\n",
            "457 0.6371584534645081 0.8125\n",
            "458 0.5249615907669067 0.9375\n",
            "459 0.5024827718734741 0.875\n",
            "460 0.5179194211959839 0.9375\n",
            "461 0.844767689704895 0.75\n",
            "462 0.9672154784202576 0.71875\n",
            "463 0.5851603150367737 0.8125\n",
            "464 1.0046541690826416 0.65625\n",
            "465 0.642841100692749 0.84375\n",
            "466 0.6631610989570618 0.8125\n",
            "467 0.76995849609375 0.875\n",
            "468 0.8090711832046509 0.8125\n",
            "469 0.8830815553665161 0.75\n",
            "470 0.8351905941963196 0.78125\n",
            "471 0.5129148960113525 0.90625\n",
            "472 0.48779013752937317 0.875\n",
            "473 0.5045632719993591 0.90625\n",
            "474 0.6093441843986511 0.875\n",
            "475 0.41048088669776917 0.9375\n",
            "476 0.8150589466094971 0.75\n",
            "477 0.7836152911186218 0.8125\n",
            "478 0.7914971113204956 0.78125\n",
            "479 0.5644443035125732 0.90625\n",
            "480 0.6023806929588318 0.875\n",
            "481 0.6255934238433838 0.84375\n",
            "482 0.7368220686912537 0.78125\n",
            "483 0.8129051923751831 0.78125\n",
            "484 0.5968756079673767 0.84375\n",
            "485 0.6896620988845825 0.84375\n",
            "486 0.7194599509239197 0.84375\n",
            "487 0.9139631390571594 0.71875\n",
            "488 0.6342819929122925 0.84375\n",
            "489 0.7339245080947876 0.875\n",
            "490 0.7794556617736816 0.75\n",
            "491 0.6855859160423279 0.78125\n",
            "492 0.8243731260299683 0.75\n",
            "493 0.7625932097434998 0.75\n",
            "494 0.43655291199684143 0.96875\n",
            "495 0.6629511117935181 0.75\n",
            "496 0.7711675763130188 0.78125\n",
            "497 0.7815059423446655 0.78125\n",
            "498 1.0724862813949585 0.65625\n",
            "499 0.5846088528633118 0.875\n",
            "500 0.35116156935691833 0.90625\n",
            "501 0.600455105304718 0.84375\n",
            "502 0.3666549026966095 0.96875\n",
            "503 0.9829046130180359 0.78125\n",
            "504 0.71280437707901 0.84375\n",
            "505 0.9046141505241394 0.78125\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7565bd95bcc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         prediciton = model(input_ids=input_ids,\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 token_type_ids=token_type_ids)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-7565bd95bcc3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             out = pretrained(input_ids=input_ids,\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 token_type_ids=token_type_ids)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1021\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kn3XUFUlBgdB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}