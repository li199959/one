{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1a_8ydLybfSOPLsgRR6XVwjg7M6DNkKD2",
      "authorship_tag": "ABX9TyMWYnZ7JMRgYlCfP4QX2r5y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li199959/one/blob/main/%E2%80%9Cbert%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB2%E2%80%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EhpxN3Gwdhgx",
        "outputId": "3d02584d-9d22-453e-a90a-c5ceab06cf37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import json, time \n",
        "from tqdm import tqdm \n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "bert_path = \"/content/drive/MyDrive/bert_model/\"    # 该文件夹下存放三个文件（'vocab.txt', 'pytorch_model.bin', 'config.json'）\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_path)   # 初始化分词器"
      ],
      "metadata": {
        "id": "ky5y_ZFBd55Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, input_masks, input_types,  = [], [], []  # input char ids, segment type ids,  attention mask\n",
        "labels = []         # 标签\n",
        "maxlen = 30      # 取30即可覆盖99%\n",
        " \n",
        "with open(\"news_title_dataset.csv\", encoding='utf-8') as f:\n",
        "    for i, line in tqdm(enumerate(f)): \n",
        "        title, y = line.strip().split('\\t')\n",
        "\n",
        "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
        "        # 根据参数会短则补齐，长则切断\n",
        "        encode_dict = tokenizer.encode_plus(text=title, max_length=maxlen, \n",
        "                                            padding='max_length', truncation=True)\n",
        "        \n",
        "        input_ids.append(encode_dict['input_ids'])\n",
        "        input_types.append(encode_dict['token_type_ids'])\n",
        "        input_masks.append(encode_dict['attention_mask'])\n",
        "\n",
        "        labels.append(int(y))\n",
        "\n",
        "input_ids, input_types, input_masks = np.array(input_ids), np.array(input_types), np.array(input_masks)\n",
        "labels = np.array(labels)\n",
        "print(input_ids.shape, input_types.shape, input_masks.shape, labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHYodgs1eLYC",
        "outputId": "af4e062b-59a4-4f69-b1da-13b0e586348a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "600it [00:00, 5815.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600, 30) (600, 30) (600, 30) (600,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 随机打乱索引\n",
        "idxes = np.arange(input_ids.shape[0])\n",
        "np.random.seed(2019)   # 固定种子\n",
        "np.random.shuffle(idxes)\n",
        "print(idxes.shape, idxes[:10])\n",
        "\n",
        "\n",
        "# 8:1:1 划分训练集、验证集、测试集\n",
        "input_ids_train, input_ids_valid, input_ids_test = input_ids[idxes[:480]], input_ids[idxes[480:540]], input_ids[idxes[540:]]\n",
        "input_masks_train, input_masks_valid, input_masks_test = input_masks[idxes[:480]], input_masks[idxes[480:540]], input_masks[idxes[540:]] \n",
        "input_types_train, input_types_valid, input_types_test = input_types[idxes[:480]], input_types[idxes[480:540]], input_types[idxes[540:]]\n",
        "\n",
        "y_train, y_valid, y_test = labels[idxes[:480]], labels[idxes[480:540]], labels[idxes[540:]]\n",
        "\n",
        "print(input_ids_train.shape, y_train.shape, input_ids_valid.shape, y_valid.shape, \n",
        "      input_ids_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2mijdSBjzH4",
        "outputId": "25fc8e92-b980-4a29-d7bf-0efbd8c3190e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(600,) [520 266 127 527 260 351 515 391 478  41]\n",
            "(480, 30) (480,) (60, 30) (60,) (60, 30) (60,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4  # 如果会出现OOM问题，减小它\n",
        "# 训练集\n",
        "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
        "                           torch.LongTensor(input_masks_train), \n",
        "                           torch.LongTensor(input_types_train), \n",
        "                           torch.LongTensor(y_train))\n",
        "train_sampler = RandomSampler(train_data)  \n",
        "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 验证集\n",
        "valid_data = TensorDataset(torch.LongTensor(input_ids_valid), \n",
        "                          torch.LongTensor(input_masks_valid),\n",
        "                          torch.LongTensor(input_types_valid), \n",
        "                          torch.LongTensor(y_valid))\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_loader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 测试集（是没有标签的）\n",
        "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
        "                          torch.LongTensor(input_masks_test),\n",
        "                          torch.LongTensor(input_types_test))\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "_WgcrPVtk_qh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义model\n",
        "class Bert_Model(nn.Module):\n",
        "    def __init__(self, bert_path, classes=3):\n",
        "        super(Bert_Model, self).__init__()\n",
        "        self.config = BertConfig.from_pretrained(bert_path)  # 导入模型超参数\n",
        "        self.bert = BertModel.from_pretrained(bert_path)     # 加载预训练模型权重\n",
        "        self.fc = nn.Linear(self.config.hidden_size, classes)  # 直接分类\n",
        "        \n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
        "        out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
        "        logit = self.fc(out_pool)   #  [bs, classes]\n",
        "        return logit"
      ],
      "metadata": {
        "id": "0MsQTgjllJ6Y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_parameter_number(model):\n",
        "    #  打印模型参数量\n",
        "    total_num = sum(p.numel() for p in model.parameters())\n",
        "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 5\n",
        "model = Bert_Model(bert_path).to(DEVICE)\n",
        "print(get_parameter_number(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REZf4QmWlXsD",
        "outputId": "57fb911e-f7ee-4228-cb50-46a85df38bc9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/bert_model/ were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 102269955, Trainable parameters: 102269955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),\n",
        "                                            num_training_steps=EPOCHS*len(train_loader))"
      ],
      "metadata": {
        "id": "DqR29ttklpaL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7rDAb3FY7Bbi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07ffde7-2da1-4693-d6be-72a11c32bf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 评估模型性能，在验证集上\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    val_true, val_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
        "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
        "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
        "            val_pred.extend(y_pred)\n",
        "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
        "    \n",
        "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
        "\n",
        "\n",
        "# 测试集没有标签，需要预测提交\n",
        "def predict(model, data_loader, device):\n",
        "    model.eval()\n",
        "    val_pred = []\n",
        "    with torch.no_grad():\n",
        "        for idx, (ids, att, tpe) in tqdm(enumerate(data_loader)):\n",
        "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
        "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
        "            val_pred.extend(y_pred)\n",
        "    return val_pred\n",
        "\n",
        "\n",
        "def train_and_eval(model, train_loader, valid_loader, \n",
        "                   optimizer, scheduler, device, epoch):\n",
        "    best_acc = 0.0\n",
        "    patience = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for i in range(epoch):\n",
        "        \"\"\"训练模型\"\"\"\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
        "        train_loss_sum = 0.0\n",
        "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
        "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)  \n",
        "            y_pred = model(ids, att, tpe)\n",
        "            loss = criterion(y_pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()   # 学习率变化\n",
        "            \n",
        "            train_loss_sum += loss.item()\n",
        "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
        "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
        "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
        "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
        "\n",
        "        \"\"\"验证模型\"\"\"\n",
        "        model.eval()\n",
        "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
        "        ## 保存最优模型\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
        "        \n",
        "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
        "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
      ],
      "metadata": {
        "id": "kRPhSVzUmT2Z"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKPOvI8wmX3U",
        "outputId": "389c0236-9463-4c1a-e73e-3b2aa3e5a673"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Running training epoch 1 *****\n",
            "Epoch 0001 | Step 0024/0120 | Loss 1.2224 | Time 1.8196\n",
            "Epoch 0001 | Step 0048/0120 | Loss 1.1440 | Time 3.7188\n",
            "Epoch 0001 | Step 0072/0120 | Loss 1.0983 | Time 5.6807\n",
            "Epoch 0001 | Step 0096/0120 | Loss 1.0439 | Time 7.3713\n",
            "Epoch 0001 | Step 0120/0120 | Loss 1.0239 | Time 8.9532\n",
            "current acc is 0.5833, best acc is 0.5833\n",
            "time costed = 10.16263s \n",
            "\n",
            "***** Running training epoch 2 *****\n",
            "Epoch 0002 | Step 0024/0120 | Loss 0.7931 | Time 1.6068\n",
            "Epoch 0002 | Step 0048/0120 | Loss 0.8066 | Time 3.1806\n",
            "Epoch 0002 | Step 0072/0120 | Loss 0.8379 | Time 4.7172\n",
            "Epoch 0002 | Step 0096/0120 | Loss 0.8012 | Time 6.3913\n",
            "Epoch 0002 | Step 0120/0120 | Loss 0.7606 | Time 8.2739\n",
            "current acc is 0.7333, best acc is 0.7333\n",
            "time costed = 9.90522s \n",
            "\n",
            "***** Running training epoch 3 *****\n",
            "Epoch 0003 | Step 0024/0120 | Loss 0.3013 | Time 1.9635\n",
            "Epoch 0003 | Step 0048/0120 | Loss 0.3519 | Time 3.5665\n",
            "Epoch 0003 | Step 0072/0120 | Loss 0.3852 | Time 5.1539\n",
            "Epoch 0003 | Step 0096/0120 | Loss 0.3955 | Time 6.7360\n",
            "Epoch 0003 | Step 0120/0120 | Loss 0.4035 | Time 8.3330\n",
            "current acc is 0.7667, best acc is 0.7667\n",
            "time costed = 9.71828s \n",
            "\n",
            "***** Running training epoch 4 *****\n",
            "Epoch 0004 | Step 0024/0120 | Loss 0.2666 | Time 1.6168\n",
            "Epoch 0004 | Step 0048/0120 | Loss 0.2334 | Time 3.5379\n",
            "Epoch 0004 | Step 0072/0120 | Loss 0.2127 | Time 5.4223\n",
            "Epoch 0004 | Step 0096/0120 | Loss 0.1922 | Time 7.3714\n",
            "Epoch 0004 | Step 0120/0120 | Loss 0.2090 | Time 8.9559\n",
            "current acc is 0.8333, best acc is 0.8333\n",
            "time costed = 10.374s \n",
            "\n",
            "***** Running training epoch 5 *****\n",
            "Epoch 0005 | Step 0024/0120 | Loss 0.0924 | Time 1.5847\n",
            "Epoch 0005 | Step 0048/0120 | Loss 0.1106 | Time 3.1528\n",
            "Epoch 0005 | Step 0072/0120 | Loss 0.1196 | Time 4.7259\n",
            "Epoch 0005 | Step 0096/0120 | Loss 0.1215 | Time 6.3296\n",
            "Epoch 0005 | Step 0120/0120 | Loss 0.1203 | Time 8.1558\n",
            "current acc is 0.8333, best acc is 0.8333\n",
            "time costed = 8.45339s \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载最优权重对测试集测试\n",
        "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
        "pred_test = predict(model, test_loader, DEVICE)\n",
        "print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(y_test, pred_test)))\n",
        "print(classification_report(y_test, pred_test, digits=4))"
      ],
      "metadata": {
        "id": "7h6u65AkmiDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998ff7f4-74e4-4cee-8c41-71659b72b89b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "15it [00:00, 61.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Test Accuracy = 0.8166666666666667 \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8929    0.8333    0.8621        30\n",
            "           1     0.8571    0.8571    0.8571        21\n",
            "           2     0.5455    0.6667    0.6000         9\n",
            "\n",
            "    accuracy                         0.8167        60\n",
            "   macro avg     0.7652    0.7857    0.7731        60\n",
            "weighted avg     0.8282    0.8167    0.8210        60\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d2b-G_38KTBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}